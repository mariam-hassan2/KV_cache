# Repository Overview

This repository includes projects and notes on KV cache and its optimization methods for efficient large language model inference.

## KV Cache Performance Demo

This is an application where you can enter a prompt and see how inference times differ **with and without key-value (KV) caching**. It’s available online, so you can try it now.

**Try it online**:  [**Here**](<https://huggingface.co/spaces/mtarek123456/kv-cache-performance-demo>)

**How to use it:** If you’re unsure, follow the step-by-step instructions   [KV Cache Performance Demo](<https://github.com/mariam-hassan2/KV_cache/tree/main/KV%20Cache%20Inference%20Demo>)

**View the code:** You can explore the full source code 
[KV Cache Performance Demo](<https://github.com/mariam-hassan2/KV_cache/tree/main/KV%20Cache%20Inference%20Demo>)

## Multi-Head Attention and KV Cache with Quantization
Implementation of Multi-Head Attention built from scratch, along with KV caching combined with quantization techniques to reduce memory usage.  
**Links:**  
- [MHA Implementation with KV Cache Quantization](<https://github.com/mariam-hassan2/KV_cache/tree/d313c7aa00ce48bcb9a5bbab3af35e7c9eee2dc5/MHA%20with%20KV%20and%20Quantization>)

## KV optimization methods Notes
Notes summarizing some of the most effective methods for KV cache optimization methods.  
**Link:** [KV optimization methods Notes](<https://github.com/mariam-hassan2/KV_cache/blob/main/KV_Cache_Optimization_Methods.pdf>)


