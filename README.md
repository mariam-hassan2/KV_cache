# Repository Overview

This repository includes projects and notes on KV cache and its optimization methods for efficient large language model inference.

## Multi-Head Attention and KV Cache with Quantization
Implementation of Multi-Head Attention built from scratch, along with KV caching combined with quantization techniques to reduce memory usage.  
**Links:**  
- [MHA Implementation with KV Cache Quantization](<https://github.com/mariam-hassan2/KV_cache/tree/d313c7aa00ce48bcb9a5bbab3af35e7c9eee2dc5/MHA%20with%20KV%20and%20Quantization>)

## KV optimization methods Notes
Notes summarizing some of the most effective methods for KV cache optimization methods.  
**Link:** [KV optimization methods Notes](<https://github.com/mariam-hassan2/KV_cache/blob/main/KV_Cache_Optimization_Methods.pdf>)

## KV Cache Performance Demo
An application that allows users to enter a prompt and compare inference time with and without KV caching.  
**Link:** [KV Cache Timing Demo](<https://github.com/mariam-hassan2/KV_cache/tree/main/KV%20Cache%20Inference%20Demo>)
