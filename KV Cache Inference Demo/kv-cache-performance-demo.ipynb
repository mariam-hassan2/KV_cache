{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gradio torch transformers accelerate -q\n\nimport gradio as gr\nimport torch\nimport time\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T21:59:21.828470Z","iopub.execute_input":"2026-02-10T21:59:21.829263Z","iopub.status.idle":"2026-02-10T21:59:42.025922Z","shell.execute_reply.started":"2026-02-10T21:59:21.829208Z","shell.execute_reply":"2026-02-10T21:59:42.024318Z"}},"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.32s/steps]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\ndef generate(text, max_tokens, use_cache):\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    torch.cuda.empty_cache()\n    \n    start = time.time()\n    with torch.no_grad():\n        output = model.generate(\n            **inputs, \n            max_new_tokens=int(max_tokens),\n            do_sample=True, \n            temperature=0.7,\n            use_cache=use_cache,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    elapsed = time.time() - start\n    \n    mem_peak = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n    result = tokenizer.decode(output[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n    \n    return result, f\"Time: {elapsed:.2f}s | Mem: {mem_peak:.2f}GB\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T21:59:42.028566Z","iopub.execute_input":"2026-02-10T21:59:42.029045Z","iopub.status.idle":"2026-02-10T21:59:42.038531Z","shell.execute_reply.started":"2026-02-10T21:59:42.028998Z","shell.execute_reply":"2026-02-10T21:59:42.037462Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"demo = gr.Interface(\n    fn=generate,\n    inputs=[\n        gr.Textbox(label=\"Prompt\", value=\"Explain KV-cache\"),\n        gr.Slider(10, 100, 30, step=10, label=\"Max Tokens\"),\n        gr.Checkbox(label=\"Use KV-Cache\", value=True)\n    ],\n    outputs=[\n        gr.Textbox(label=\"Generation\", lines=6),\n        gr.Textbox(label=\"Performance\")\n    ],\n    title=\"KV Cache Performance Demo\",\n    description=\"Toggle cache to see speedup/memory diff!\"\n)\n\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T21:59:42.040065Z","iopub.execute_input":"2026-02-10T21:59:42.041202Z","iopub.status.idle":"2026-02-10T22:03:46.803567Z","shell.execute_reply.started":"2026-02-10T21:59:42.041144Z","shell.execute_reply":"2026-02-10T22:03:46.801508Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\n* Running on public URL: https://52c83927767c58aad7.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://52c83927767c58aad7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7862 <> https://52c83927767c58aad7.gradio.live\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":22}]}