{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14788561,"sourceType":"datasetVersion","datasetId":9454351}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tiktoken\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\n\ndef create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True):\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    return dataloader\n\n\nwith open(\"/kaggle/input/small-text-sample-txt/small-text-sample.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nencoded_text = tokenizer.encode(raw_text)\n\nvocab_size = 50257\noutput_dim = 256\nmax_len = 1024\ncontext_length = max_len\n\n\ntoken_embedding_layer = nn.Embedding(vocab_size, output_dim)\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n\nmax_length = 4\ndataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:45:13.969915Z","iopub.execute_input":"2026-02-10T20:45:13.970150Z","iopub.status.idle":"2026-02-10T20:45:22.772643Z","shell.execute_reply.started":"2026-02-10T20:45:13.970128Z","shell.execute_reply":"2026-02-10T20:45:22.771856Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"for batch in dataloader:\n    x, y = batch\n\n    token_embeddings = token_embedding_layer(x)\n    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n\n    input_embeddings = token_embeddings + pos_embeddings\n\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:45:22.774066Z","iopub.execute_input":"2026-02-10T20:45:22.774468Z","iopub.status.idle":"2026-02-10T20:45:22.820628Z","shell.execute_reply.started":"2026-02-10T20:45:22.774442Z","shell.execute_reply":"2026-02-10T20:45:22.820108Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Simple Multi-Head Attention","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SelfAttention(nn.Module):\n    def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('mask',torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self,x):\n        b,n_tokens ,d_in = x.shape\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_query(x)\n        # attention scores = QK^T\n        attn_scores =  queries @ keys.transpose(1,2)\n        attn_scores.masked_fill(self.mask.bool()[:n_tokens,:n_tokens],-torch.inf)\n        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        context_vec = attn_weights@values\n        return context_vec\n\nclass MultiHeadAttentionWrapper(nn.Module):\n     def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n         super().__init__()\n         self.heads = nn.ModuleList(SelfAttention(d_in,d_out,context_length,dropout) for _ in range(num_heads))\n         self.out_proj = nn.Linear(d_out*num_heads,d_out*num_heads)\n\n     def forward(self,x):\n        context_vec = torch.cat([head(x) for head in self.heads],dim=-1)\n        return context_vec\n        \n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:45:22.821453Z","iopub.execute_input":"2026-02-10T20:45:22.821736Z","iopub.status.idle":"2026-02-10T20:45:22.829785Z","shell.execute_reply.started":"2026-02-10T20:45:22.821708Z","shell.execute_reply":"2026-02-10T20:45:22.829178Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import time\ntorch.manual_seed(123)\ncontext_length = max_length\nd_in = output_dim\nnum_heads = 2\nd_out = d_in//num_heads\nmha = MultiHeadAttentionWrapper(d_in,d_out,context_length,0.0,num_heads)\nbatch=input_embeddings\ncontext_vec = mha(batch)\nprint(context_vec.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:46:34.837999Z","iopub.execute_input":"2026-02-10T20:46:34.838373Z","iopub.status.idle":"2026-02-10T20:46:34.865560Z","shell.execute_reply.started":"2026-02-10T20:46:34.838337Z","shell.execute_reply":"2026-02-10T20:46:34.864708Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 4, 256])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Multi-Head Attention with KV cache","metadata":{}},{"cell_type":"code","source":"import time\nimport tiktoken\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False,max_seq_len=None,window_size=None):\n        super().__init__()\n        self.d_out = d_out\n        self.num_heads=num_heads\n        self.head_dim = d_out//self.num_heads ## Reduce the projection dim to match desired output dim\n        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out,d_out)  #linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n\n\n        ##\n        self.max_seq_len = max_seq_len or context_length\n        self.window_size = window_size or self.max_seq_len\n        self.register_buffer(\"cache_k\",None,persistent=False)\n        self.register_buffer(\"cache_v\",None,persistent=False)\n        ##\n\n    def forward(self,x, use_cache=False):\n        b,num_tokens,d_in = x.shape\n        if use_cache:\n            # to prevent self.ptr_cur became negative\n            assert num_tokens <= self.window_size, (\n                f\"Input chunk size ({num_tokens}) exceeds KV cache window size ({self.window_size}). \"\n            )\n            \n        keys_new = self.W_key(x)\n        values_new = self.W_value(x)\n        queries_new = self.W_query(x)\n\n        keys_new = keys_new.view(b,num_tokens,self.num_heads,self.head_dim)\n        values_new = values_new.view(b,num_tokens,self.num_heads,self.head_dim)\n        queries_new = queries_new.view(b,num_tokens,self.num_heads,self.head_dim)\n\n        keys_new = keys_new.transpose(1,2)\n        values_new = values_new.transpose(1,2)\n        queries_new = queries_new.transpose(1,2)\n\n        #####\n        #new\n        ########\n        if use_cache:\n            if self.cache_k is None or self.cache_k.size(0)!=b:\n                self.cache_k = torch.zeros(b,self.num_heads,self.window_size,self.head_dim,device = x.device)\n                self.cache_v = torch.zeros_like(self.cache_k)\n                self.ptr_cur = 0\n            # if incoming chunk would overlfow, discard oldest token\n\n            if self.ptr_cur +num_tokens > self.window_size:\n                overflow = self.ptr_cur + num_tokens - self.window_size\n                # shift everythign left by overflow\n                self.cache_k[:,:,:-overflow,:] = self.cache_k[:,:,overflow:,:].clone()\n                self.cache_v[:,:,:-overflow,:] = self.cache_v[:,:,overflow:,:].clone()\n                self.ptr_cur -= overflow\n            \n            self.cache_k[:,:,self.ptr_cur:self.ptr_cur+num_tokens,:] = keys_new\n            self.cache_v[:,:,self.ptr_cur:self.ptr_cur+num_tokens,:] = values_new\n            self.ptr_cur+=num_tokens\n\n\n            keys = self.cache_k[:,:,:self.ptr_cur,:]\n            values = self.cache_v[:,:,:self.ptr_cur,:]\n        else:\n            keys, values = keys_new,values_new\n            self.ptr_cur=0\n\n        attn_scores = queries_new@keys.transpose(2,3)\n\n        K = attn_scores.size(-1)\n\n        if num_tokens==K:\n            casual_mask = torch.triu(torch.ones(num_tokens,K,device=x.device,dtype=torch.bool),diagonal=1)\n        else:\n            offset = K-num_tokens\n            row_idx = torch.arange(num_tokens,device=x.device).unsqueeze(1)\n            col_idx = torch.arange(K,device=x.device).unsqueeze(0)\n            casual_mask = row_idx+offset<col_idx\n\n\n        # use the mask to fill the attention scores\n        attn_scores.masked_fill(casual_mask.unsqueeze(0).unsqueeze(0),-torch.inf)\n        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        context_vec = (attn_weights@values).transpose(1,2)\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)\n        return context_vec\n\n    def reset_cache(self):\n        self.cache_k,self.cache_v = None,None\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:46:06.023571Z","iopub.execute_input":"2026-02-10T20:46:06.024535Z","iopub.status.idle":"2026-02-10T20:46:06.038014Z","shell.execute_reply.started":"2026-02-10T20:46:06.024494Z","shell.execute_reply":"2026-02-10T20:46:06.037186Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nmax_len = 2048\ncontext_length = max_len\nd_in = output_dim\nnum_heads = 2\nd_out = d_in//num_heads\nmha = MultiHeadAttention(d_in,d_out,context_length,0.0,num_heads)\nbatch=input_embeddings\ncontext_vec = mha(batch,False)\nprint(context_vec.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:46:49.014202Z","iopub.execute_input":"2026-02-10T20:46:49.014788Z","iopub.status.idle":"2026-02-10T20:46:49.023269Z","shell.execute_reply.started":"2026-02-10T20:46:49.014759Z","shell.execute_reply":"2026-02-10T20:46:49.022512Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 4, 128])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Quantization\n","metadata":{}},{"cell_type":"code","source":"import time\nimport tiktoken\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False,max_seq_len=None,window_size=None):\n        super().__init__()\n        self.d_out = d_out\n        self.num_heads=num_heads\n        self.head_dim = d_out//self.num_heads \n        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out,d_out)\n        self.dropout = nn.Dropout(dropout)\n\n        # Cache buffers\n        self.max_seq_len = max_seq_len or context_length\n        self.window_size = window_size or self.max_seq_len\n        self.register_buffer(\"cache_k\", None, persistent=False)\n        self.register_buffer(\"cache_v\", None, persistent=False)\n        self.register_buffer(\"cache_kq\", None, persistent=False)\n        self.register_buffer(\"cache_vq\", None, persistent=False)\n        self.ptr_cur = 0\n        \n        # Quant\n        self.quant_bits = 8\n        self.register_buffer(\"k_scale\", torch.tensor(1.0))\n        self.register_buffer(\"v_scale\", torch.tensor(1.0))\n        \n    def quantize(self, tensor):\n        scale = tensor.abs().max(dim=-1, keepdim=True)[0] / 127.0\n        scale = scale.clamp(min=1e-5)\n        q = torch.round(tensor / scale).clamp(-128, 127).to(torch.int8)\n        return q, scale\n    \n    def dequantize(self, q_int8, scale):\n        return q_int8.to(torch.float16) * scale\n        \n    def reset_cache(self):\n        self.cache_k = self.cache_v = self.cache_kq = self.cache_vq = None\n        self.ptr_cur = 0\n        \n    def forward(self, x, use_cache=False, use_quantize=False):\n        b, num_tokens, _ = x.shape\n        \n        # Compute QKV\n        queries_new = self.W_query(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n        keys_new    = self.W_key(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n        values_new  = self.W_value(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n\n        if use_cache:\n            assert num_tokens <= self.window_size, f\"Chunk {num_tokens} > window {self.window_size}\"\n            \n            if self.cache_k is None or self.cache_k.shape[0] != b:\n                self.cache_k  = torch.zeros(b, self.num_heads, self.window_size, self.head_dim, device=x.device, dtype=torch.float16)\n                self.cache_v  = torch.zeros_like(self.cache_k)\n                self.cache_kq = torch.zeros(b, self.num_heads, self.window_size, self.head_dim, device=x.device, dtype=torch.int8)\n                self.cache_vq = torch.zeros_like(self.cache_kq)\n                self.ptr_cur = 0\n\n            # Overflow handling\n            if self.ptr_cur + num_tokens > self.window_size:\n                overflow = self.ptr_cur + num_tokens - self.window_size\n                self.cache_k [:,:,:-overflow,:] = self.cache_k [:,:,overflow:,:].clone()\n                self.cache_v [:,:,:-overflow,:] = self.cache_v [:,:,overflow:,:].clone()\n                self.cache_kq[:,:,:-overflow,:] = self.cache_kq[:,:,overflow:,:].clone()\n                self.cache_vq[:,:,:-overflow,:] = self.cache_vq[:,:,overflow:,:].clone()\n                self.ptr_cur -= overflow\n\n            # Store new keys/values\n            if use_quantize:\n                kq_new, k_scale_new = self.quantize(keys_new.float())\n                vq_new, v_scale_new = self.quantize(values_new.float())\n                self.cache_kq[:,:,self.ptr_cur:self.ptr_cur+num_tokens,:] = kq_new\n                self.cache_vq[:,:,self.ptr_cur:self.ptr_cur+num_tokens,:] = vq_new\n                keys = self.dequantize(self.cache_kq[:,:,:self.ptr_cur+num_tokens,:], self.k_scale)\n                values = self.dequantize(self.cache_vq[:,:,:self.ptr_cur+num_tokens,:], self.v_scale)\n                self.k_scale = 0.9 * self.k_scale + 0.1 * k_scale_new.mean()\n                self.v_scale = 0.9 * self.v_scale + 0.1 * v_scale_new.mean()\n            else:\n                keys_new_fp16 = keys_new.to(torch.float16)\n                values_new_fp16 = values_new.to(torch.float16)\n                self.cache_k[:,:,self.ptr_cur:self.ptr_cur+num_tokens,:] = keys_new_fp16\n                self.cache_v[:,:,self.ptr_cur:self.ptr_cur+num_tokens,:] = values_new_fp16\n                keys = self.cache_k[:,:,:self.ptr_cur+num_tokens,:]\n                values = self.cache_v[:,:,:self.ptr_cur+num_tokens,:]\n\n            self.ptr_cur += num_tokens\n        else:\n            keys, values = keys_new.to(torch.float16), values_new.to(torch.float16)\n            self.ptr_cur = 0\n\n        # Cast queries to FP16\n        queries_new = queries_new.to(torch.float16)\n\n        # Attention (all FP16)\n        attn_scores = torch.matmul(queries_new, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        \n        # Causal mask\n        K = keys.size(-2)\n        if num_tokens == K:\n            causal_mask = torch.triu(torch.ones(num_tokens, K, device=x.device, dtype=torch.bool), diagonal=1)\n        else:\n            offset = K - num_tokens\n            row_idx = torch.arange(num_tokens, device=x.device).unsqueeze(1)\n            col_idx = torch.arange(K, device=x.device).unsqueeze(0)\n            causal_mask = row_idx + offset < col_idx\n\n        attn_scores.masked_fill_(causal_mask.unsqueeze(0).unsqueeze(0), -torch.inf)\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        context_vec = torch.matmul(attn_weights, values).transpose(1, 2)\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        \n        # *** FIX: Cast back to FP32 for out_proj ***\n        context_vec = context_vec.to(torch.float32)\n        context_vec = self.out_proj(context_vec)\n        \n        return context_vec\n\nif __name__ == \"__main__\":\n    torch.manual_seed(123)\n    context_length = 1024\n    d_in = 512\n    num_heads = 8\n    d_out = d_in\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    attn = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads).to(device)\n    long_input = torch.randn(1, 1024, d_in, device=device)\n    \n    print(\"=== NON-QUANTIZED (FP16 Cache) ===\")\n    attn.reset_cache()\n    out_nonquant = attn(long_input, use_cache=True, use_quantize=False)\n    fp16_mem = attn.cache_k.numel() * attn.cache_k.element_size() / 1e6\n    print(f\"Cache K: {attn.cache_k.dtype} {fp16_mem:.1f}MB (K+V: {fp16_mem*2:.1f}MB)\")\n    print(f\"Output: {out_nonquant.shape}\")\n\n    print(\"\\n=== QUANTIZED (INT8 Cache) ===\")\n    attn.reset_cache()\n    out_quant = attn(long_input, use_cache=True, use_quantize=True)\n    int8_mem = attn.cache_kq.numel() * attn.cache_kq.element_size() / 1e6\n    print(f\"Cache Kq: {attn.cache_kq.dtype} {int8_mem:.1f}MB (Kq+Vq: {int8_mem*2:.1f}MB)\")\n    print(f\"Output: {out_quant.shape}\")\n    \n    match = torch.allclose(out_nonquant, out_quant, atol=1e-2)\n    print(f\"\\nOutputs match: {match}\")\n    print(f\"Memory savings: {100*(1-int8_mem/fp16_mem):.0f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:46:52.370815Z","iopub.execute_input":"2026-02-10T20:46:52.371525Z","iopub.status.idle":"2026-02-10T20:46:53.204015Z","shell.execute_reply.started":"2026-02-10T20:46:52.371490Z","shell.execute_reply":"2026-02-10T20:46:53.203398Z"}},"outputs":[{"name":"stdout","text":"=== NON-QUANTIZED (FP16 Cache) ===\nCache K: torch.float16 1.0MB (K+V: 2.1MB)\nOutput: torch.Size([1, 1024, 512])\n\n=== QUANTIZED (INT8 Cache) ===\nCache Kq: torch.int8 0.5MB (Kq+Vq: 1.0MB)\nOutput: torch.Size([1, 1024, 512])\n\nOutputs match: False\nMemory savings: 50%\n","output_type":"stream"}],"execution_count":11}]}