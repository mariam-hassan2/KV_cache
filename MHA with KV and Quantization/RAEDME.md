# Multi-Head Attention from scratch With KV and Quantization

**Base**: Sebastian Raschka's [LLMs-from-Scratch](https://github.com/rasbt/LLMs-from-scratch) Ch4  

**Enhancements**:  
âœ… INT8 KV cache (50% memory)


